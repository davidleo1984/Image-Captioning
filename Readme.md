![image-captioning](\images\image-captioning.png)

<center>Image Captioning Model</center>


## Project Overview

In this project, we created a neural network architecture to automatically generate captions from images.

After using the Microsoft Common Objects in COntext [(MS COCO) dataset](http://cocodataset.org/#home) to train your network, we tested the network on novel images!

## Project Instructions

The project is structured as a series of Jupyter notebooks that are designed to be completed in sequential order:

* 0_Dataset.ipynb
* 1_Preliminaries.ipynb
* 2_Training.ipynb
* 3_Inference.ipynb

## Evaluation

The project has been reviewed by a Udacity reviewer against the project [rubric](https://review.udacity.com/#!/rubrics/1427/view).

## Project template from Udaicty

It is a known issue that the COCO dataset is not well supported for download on Windows, if you would like to refer to the project code, you may look at [this version (in PyTorch 0.4.0)](https://github.com/udacity/CVND---Image-Captioning-Project) at the linked Github repo.

## Results

Please see the final results in `2_Training.html` and `3_Inference.html`.
